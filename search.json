[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "2023 Turkish Presidential Election, Part 1: The Table of Six",
    "section": "",
    "text": "As it stands, there are three potential candidates for the coalition: Kemal Kılıçdaroğlu, the leader of the Republican People’s Party (CHP), Ekrem İmamoğlu, the mayor of Istanbul, and Mansur Yavaş, the mayor of Ankara. The other five parties in the coalition seem to be in favor of Kılıçdaroğlu, but IYI Party’s leader Meral Akşener made it clear that she’d rather have İmamoğlu or Yavaş as the candidate, as the two regarded to be more popular than Kılıçdaroğlu.\nIt is quite hard to forecast elections in Turkey, as the polls are often wrong, and the election results are often unexpected. However, let’s try our best to analyze the situation with what we have publicly available. Here are the packages we’ll be using:\n\n# For web scraping\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\n# For data manipulation\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\n# style\nplt.style.use('seaborn-darkgrid')\n\nFor today’s web scraping, we’ll be using the Wikipedia page for the election surveys. I will be greedy and scrape all the tables in the page, and then I’ll filter the ones that are relevant to us. Here’s the code:\n\nlink = \"https://tr.wikipedia.org/wiki/2023_T%C3%BCrkiye_cumhurba%C5%9Fkanl%C4%B1%C4%9F%C4%B1_se%C3%A7imi_i%C3%A7in_yap%C4%B1lan_anketler\"\npage = requests.get(link)\nsoup = bs(page.content, \"html.parser\")\ntables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n\nNow, we’ll filter the tables that are relevant to us. We’ll be looking for tables that have the following columns:\n\ndf_list = []\ndef get_df(table):\n    rows = table.find_all(\"tr\")\n    # Get the header\n    header = [cell.text.strip() for cell in rows[0].find_all(\"th\")]\n    # Get the data\n    data = []\n    for row in rows:\n        if row.find_all(\"td\"):\n            # td or th\n            d = [cell.text.strip() for cell in row.find_all(recursive=False)]\n            if len(d) != len(header):\n                print(\"Error: \", d)\n                continue\n\n            data.append(d)\n\n    df = pd.DataFrame(data, columns=header)\n    return df\n\nfor table in tables:\n    df_list.append(get_df(table))\n\nError:  ['17–20 Eyl', 'MetroPOLL', '2.119', '42,1', '–', '16,9', '22,5', '12,4[no 1]', '–', '–', '–', '–', '–', '–', '–', '–', '19,6']\nError:  ['24 Şub', 'Polimetre[no 2]', '–', '38,5', '35,1', '9,3', '–', '8,1', '–', '–', '–', '–', '9,0', '3,4']\nError:  ['8 Eki', 'Polimetre[no 2]', '–', '46,2', '34,7', '9,5', '8,4', '–', '–', '–', '–', '1,2', '11,5']\nError:  ['Haziran', 'ORC', '–', '56,7[no 3]', '27,7', '8,4', '6,5', '–', '–', '–', '0,5', '0,2', '29,0']\n\n\nNow, we have a list of dataframes, and we can concatenate them into one dataframe:\n\n# Filter the dataframes with 7 columns\ndf_list = [df for df in df_list if len(df.columns) == 7]\n\n# merge all dataframes based on \"Tarih\" and \"Anket şirketi\" columns\nfor df in df_list:\n    candidate = df.columns[4].split(\"\\n\")[0]\n    df.columns = df.columns[:3].tolist() + [f'Erdoğan_{candidate}', candidate, f'Kararsız_{candidate}', f'Fark_{candidate}']\nprint(f'Total number of dataframes: {len(df_list)}')\n\nTotal number of dataframes: 19\n\n\nNow, we have a dataframe with all the data we need. Let’s take a look at the first five rows:\n\n# row merge dataframes with the same columns\ntemp = []\nfor df in df_list:\n    if df.columns.to_list() in [df2.columns.to_list() for df2 in temp]:\n        for i in range(len(temp)):\n            if df.columns.to_list() == temp[i].columns.to_list():\n                # df2 = pd.concat([df2, df], axis=0)\n                temp[i] = pd.concat([temp[i], df], axis=0)\n    else:\n        temp.append(df)\n\nprint(f'Total number of dataframes after merging: {len(temp)}')\n\nTotal number of dataframes after merging: 7\n\n\n\nfinal_df = temp[0]\nfor d in temp[1:]:\n    # merge on \"Tarih\", \"Anket şirketi\", \"Örneklem\"\n    final_df = pd.merge(final_df, d, on=[\"Tarih\", \"Anket şirketi\", \"Örneklem\"], how=\"outer\")\n\n\n# function to parse month in Turkish\ndef parse_month_tr(month):\n    list_tr = [\"Ocak\", \"Şubat\", \"Mart\", \"Nisan\", \"Mayıs\", \"Haziran\", \"Temmuz\", \"Ağustos\", \"Eylül\", \"Ekim\", \"Kasım\", \"Aralık\"]\n    return list_tr.index(month) + 1\n\n# tarih means data\ndef parse_tarih(tarih):\n    t = tarih\n    tarih = str(tarih).replace('–', ' ').replace('-', ' ').split()\n    numbers = [int(e) for e in tarih if e.isdigit()]\n    words = [e for e in tarih if not e.isdigit()]\n\n    temp = []\n    if len(words) == 1 and numbers:\n        for j in numbers:\n            temp.append(datetime(2023, parse_month_tr(words[0]), j))\n    elif len(words) == 2:\n        for i,j in zip(words, numbers):\n            temp.append(datetime(2023, parse_month_tr(i), j))\n    elif len(words) == 1 and not numbers:\n        temp.append(datetime(2023, parse_month_tr(words[0]), 1))\n        temp.append(datetime(2023, parse_month_tr(words[0]) % 12 + 1, 1))\n\n    if temp:\n        return temp\n    else:\n        return ['hey', numbers, words, t]\n    \nprint(f'Unparsed dates: {final_df[\"Tarih\"].apply(lambda x: parse_tarih(x)).apply(lambda x: x[0] == \"hey\").sum()}')\n\nfinal_df['Tarih'] = final_df['Tarih'].apply(parse_tarih)\nfinal_df['Başlangıç'] = final_df['Tarih'].apply(lambda x: x[0])\nfinal_df['Bitiş'] = final_df['Tarih'].apply(lambda x: x[-1])\n\n# Orneklem means sample size\ndef parse_orneklem(orneklem):\n    if orneklem == \"–\":\n        return -1\n    orneklem = orneklem.replace('.', '').replace(',', '')\n    if orneklem.isdigit():\n        return int(orneklem)\n    else:\n        return ['hey', orneklem]\n\nfinal_df['Örneklem'] = final_df['Örneklem'].apply(parse_orneklem)\nfinal_df['Anket şirketi'] = final_df['Anket şirketi'].astype('category')\n\n# For now let's drop Gül, İnce, Babacan, and Akşener\ncols_drop = [col for col in final_df.columns if \"Gül\" in col or \"İnce\" in col or \"Akşener\" in col or \"Babacan\" in col] + ['Tarih']\nfinal_df = final_df.drop(cols_drop, axis=1)\n\n# Reorder columns\ncols = ['Başlangıç', 'Bitiş'] + [col for col in final_df.columns if col not in ['Başlangıç', 'Bitiş']]\nfinal_df = final_df[cols]\n\n# drop all rows with NaN values\nfinal_df = final_df.dropna()\n\ndef parse_pct(pct):\n    pct = pct.replace(',', '.')\n    if pct == \"–\":\n        return -1\n    return float(pct)\n\ninfo_col = ['Başlangıç', 'Bitiş', 'Anket şirketi', 'Örneklem']\nnum_cols = [col for col in final_df.columns if col not in info_col]\nfinal_df[num_cols] = final_df[num_cols].applymap(parse_pct)\n\n# Make fark cols negative if Erdoğan has more votes\nfor cand in ['İmamoğlu', 'Yavaş', 'Kılıçdaroğlu']:\n    idx = final_df[final_df[f'Erdoğan_{cand}'] > final_df[cand]].index\n    final_df.loc[idx, f'Fark_{cand}'] = -final_df.loc[idx, f'Fark_{cand}']\n\nfinal_df.head()\n\n# Fixing the years\nreal_year = 2023\nchanged = False\nfor i in final_df.index[1:]:\n    if final_df.loc[i, 'Başlangıç'].month == 12 and changed == False:\n        real_year -= 1\n        changed = True\n    elif final_df.loc[i, 'Başlangıç'].month == 1 and changed == True:\n        changed = False\n\n    final_df.loc[i, 'Başlangıç'] = final_df.loc[i, 'Başlangıç'].replace(year=real_year)\n    final_df.loc[i, 'Bitiş'] = final_df.loc[i, 'Bitiş'].replace(year=real_year)\n    if final_df.loc[i, 'Bitiş'].month < final_df.loc[i, 'Başlangıç'].month:\n        final_df.loc[i, 'Bitiş'] = final_df.loc[i, 'Bitiş'].replace(year=real_year + 1)\n\n\n# sort df with respect to Başlangıç\nfinal_df = final_df.sort_values(by='Başlangıç')\nfark_cols = [col for col in final_df.columns if \"Fark\" in col]\n\n# Scatter plot\nfig, ax = plt.subplots()\nfor col in fark_cols:\n    # plot the points\n    ax.scatter(final_df['Başlangıç'], final_df[col], label=col)\n    # smooth the line\n    ax.plot(final_df['Başlangıç'], final_df[col].ewm(alpha=0.1).mean(), label=col + ' (EMA)')\n\n\nfor i in final_df.index:\n\n    y = -20 + np.random.randint(0, 7)\n    ax.axvline(final_df.loc[i, 'Başlangıç'], color='black', alpha=0.2, linestyle='--')\n    ax.annotate(final_df.loc[i, 'Anket şirketi'], (final_df.loc[i, 'Başlangıç'], y), (0, 20), textcoords='offset points', va='bottom', ha='center', fontsize=8, rotation=90)\n\n\n# annotate source wikipedia\nax.annotate('Kaynak: Vikipedi - 2023 Türkiye cumhurbaşkanlığı seçimi için yapılan anketler', (0,0), (0, -50), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.annotate('Toplam Anket sayısı: ' + str(len(final_df)), (0,0), (0, -70), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.legend()\n# tight layout\nplt.tight_layout()\nplt.show()\n\n# save the plot\nfig.savefig('anketler.png', dpi=300, bbox_inches='tight')\n\nUnparsed dates: 0"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cem Sirin",
    "section": "",
    "text": "Ciao! I’m a MSc candidate in Data Science @ Sapienza University of Rome. I am passionate about data science and quantitative finance. I aim to combine meticulousness of econometric analysis with the flexibility of machine learning.\nCheck out my blog, where I share my thoughts on data science, quantitative finance and other topics that I find interesting."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sapienza University of Rome | Rome, IT\nMSc in Data Science | Sept 2021 - Present \nErasmus University Rotterdam | Rotterdam, NL\nDouble BSc in Economics and Econometrics | Sept 2018 - June 2022\nRobert College | Istanbul, TR\nHigh School Diploma | Sept 2014 - June 2018"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nSapienza University of Rome | Rome, IT\nMSc in Data Science | Sept 2021 - Present \nErasmus University Rotterdam | Rotterdam, NL\nDouble BSc in Economics and Econometrics | Sept 2018 - June 2022\nRobert College | Istanbul, TR\nHigh School Diploma | Sept 2014 - June 2018"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nErasmus School of Economics | Rotterdam, NL\nTeaching Assistant | September 2021 - June 2022"
  },
  {
    "objectID": "posts/secimler1/index.html",
    "href": "posts/secimler1/index.html",
    "title": "2023 Turkish Presidential Election, Part 1: The Table of Six",
    "section": "",
    "text": "As it stands, there are three potential candidates for the coalition: Kemal Kılıçdaroğlu, the leader of the Republican People’s Party (CHP), Ekrem İmamoğlu, the mayor of Istanbul, and Mansur Yavaş, the mayor of Ankara. The other five parties in the coalition seem to be in favor of Kılıçdaroğlu, but IYI Party’s leader Meral Akşener made it clear that she’d rather have İmamoğlu or Yavaş as the candidate, as the two regarded to be more popular than Kılıçdaroğlu.\nIt is quite hard to forecast elections in Turkey, as the polls are often wrong, and the election results are often unexpected. However, let’s try our best to analyze the situation with what we have publicly available. Here, I am uploading the data I have scraped from Wikipedia, you can find the code here.\n\n# Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Style\nplt.style.use('seaborn-darkgrid')\n# Read the data\nfinal_df = pd.read_pickle('polls.pkl')\n\n# Fark meaning difference, we are only interested in the difference between candidates and the incumbent (Erdoğan)\nfark_cols = [col for col in final_df.columns if \"Fark\" in col]\n\n# Scatter plot\nfig, ax = plt.subplots(figsize=(20, 10))\nfor col in fark_cols:\n    # plot the points\n    ax.scatter(final_df['Başlangıç'], final_df[col], label=col)\n    # smooth the line with exponential moving average\n    ax.plot(final_df['Başlangıç'], final_df[col].ewm(alpha=0.1).mean(), label=col + ' (EMA)')\n\n\nfor i in final_df.index:\n    y = -20 + np.random.randint(0, 7)\n    ax.axvline(final_df.loc[i, 'Başlangıç'], color='black', alpha=0.2, linestyle='--')\n    ax.annotate(final_df.loc[i, 'Anket şirketi'], (final_df.loc[i, 'Başlangıç'], y), (0, 20), textcoords='offset points', va='bottom', ha='center', fontsize=8, rotation=90)\n\n\n# annotate source wikipedia\nax.annotate('Source: Wikipedia - Opinion polling for the 2023 Turkish presidential election', (0,0), (0, -50), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.annotate('#Polls: ' + str(len(final_df)), (0,0), (0, -70), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.legend()\nplt.show()"
  },
  {
    "objectID": "posts/overview/index.html",
    "href": "posts/overview/index.html",
    "title": "An Overview of My Blog",
    "section": "",
    "text": "This will be an overview of my blog.\nThis blog is a place for me to share my thoughts and ideas on the world of data science and quantitative finance. I will update this post as I add more content to the blog. I aim to compartmentalize my posts, depending on the topic."
  },
  {
    "objectID": "posts/overview/index.html#quantitative-finance",
    "href": "posts/overview/index.html#quantitative-finance",
    "title": "An Overview of My Blog",
    "section": "Quantitative Finance",
    "text": "Quantitative Finance\nWhen it comes to quantitative finance, there is no clear set of topics that it covers. Traditionally, it leans more towards topics like Risk Management, Portfolio Management, and Asset Pricing. Although these topics are still relevant, they sprout from a more traditional view of finance as the main goal is to decrease risk and uphold a high expected return. This goal is formed by the idea that all financial assets obey to Risk-Return tradeoff, that is, the higher the expected return, the higher the risk.\nAlthough, empirically this is seems true in the long-term, it does not necessarily mean that we can not achieve higher returns in the short-term. With the rise of machine learning and more interdisciplinary approaches, many new topics have emerged. The new goal is to increase returns in the short-term, while still maintaining a low risk profile. This is where topics like algorithmic trading and high-frequency trading come into play, and which I will cover in this blog.\nSince these fields are quite new, there are no clear compartmentalized topics. One of the most influential books in this field is Advances in Financial Machine Learning by Marcos Lopez de Prado. I think the chapters in this book is a good way of compartmentalizing the topics in this field, so I will follow the same structure. And, while on the topic of books, I also quite like Machine Learning for Factor Investing by Coqueret and Guida, which is also available free online. It is a “bookdown”, which is a book written in R Markdown and I like its User Interface a lot. Anyways, here is the list of topics I will cover in this blog (I will update this list as I add more content :D ):\n\nLabelling"
  },
  {
    "objectID": "index.html#quantitative-finance",
    "href": "index.html#quantitative-finance",
    "title": "Cem Sirin",
    "section": "Quantitative Finance",
    "text": "Quantitative Finance\nWhen it comes to quantitative finance, there is no clear set of topics that it covers. Traditionally, it leans more towards topics like Risk Management, Portfolio Management, and Asset Pricing. Although these topics are still relevant, they sprout from a more traditional view of finance as the main goal is to decrease risk and uphold a high expected return. This goal is formed by the idea that all financial assets obey to Risk-Return tradeoff, that is, the higher the expected return, the higher the risk.\nAlthough, empirically this is seems true in the long-term, it does not necessarily mean that we can not achieve higher returns in the short-term. With the rise of machine learning and more interdisciplinary approaches, many new topics have emerged. The new goal is to increase returns in the short-term, while still maintaining a low risk profile. This is where topics like algorithmic trading and high-frequency trading come into play, and which I will cover in this blog.\nSince these fields are quite new, there are no clear compartmentalized topics. One of the most influential books in this field is Advances in Financial Machine Learning by Marcos Lopez de Prado. I think the chapters in this book is a good way of compartmentalizing the topics in this field, so I will follow the same structure. And, while on the topic of books, I also quite like Machine Learning for Factor Investing by Coqueret and Guida, which is also available free online. It is a “bookdown”, which is a book written in R Markdown and I like its User Interface a lot. Anyways, here is the list of topics I will cover in this blog (I will update this list as I add more content :D ):\n\nLabelling"
  },
  {
    "objectID": "posts/3pleB/index.html",
    "href": "posts/3pleB/index.html",
    "title": "The triple barrier method",
    "section": "",
    "text": "Introducing the triple barrier method\n\nIntuition on consistency\nIntuition on barrier heights(#intuition-on-barrier-heights\n\nImplementation on R\nReferences\n\nThere are some existing posts/articles about the triple barrier method. They are not very detailed, and I will write a quite indepth one here. So, I will go over the method very exhaustively. I hope that you can just Ctrl + F (or Cmd + F on Mac) to find the part that you are interested in."
  },
  {
    "objectID": "posts/3pleB/index.html#introducing-the-triple-barrier-method",
    "href": "posts/3pleB/index.html#introducing-the-triple-barrier-method",
    "title": "The triple barrier method",
    "section": "Introducing the triple barrier method",
    "text": "Introducing the triple barrier method\nThe triple barrier method is a method to label data for asset price prediction. The method is coined by De Prado (2018), and it has been gaining popularity in the quantitative finance community. The method may seem foreign for the academic community, but it is actually what day traders have been doing manually for a long time. When a day trader places an order, they set a certain price point to trigger automatically to either take their profit or stop their loss.\n\n\n\nExample of a stop loss and take profit order, explained on Forex Springboard\n\n\nAbove you can see the 2 horizontal barriers that are placed to trigger exit orders. You may ask where is third barrier of the triple barrier method. The third barrier will be a vertical barrier, which is the time horizon \\(h\\). The time horizon is the maximum time that the position can be held. The label \\(y_t\\) for a given time \\(t\\) is defined as follows:\n\\[\ny_t=\\left\\{\\begin{array}{cl}\n-1 & \\text { if hit upper barrier } \\\\\n0 & \\text { if hit vertical barrier } \\\\\n1 & \\text { if hit lower barrier }\n\\end{array}\\right.\n\\]\n\nIntuition on consistency\nNow, let me wear my statistician hat and make some remarks. After we have our labels, we want to believe that the label \\(y_t\\) for time \\(t\\) is depends on some conditional probability distribution \\(f(y_t|x_t)\\) for given futures \\(x_t\\). Since, we are constructing our own labels, it relies on us to construct sound labels. If some orders trigger profit in 5 minutes and some in 5 days, the labels would not be consistent. By consistency, I mean that we should expect to have similar market conditions for each label class. Our predictive futures \\(x_t\\) captures the market conditions at time \\(t\\). If the market conditions are drastically different for each label class, the conditional probability distribution \\(f(y_t|x_t)\\) would not have much predictive power.\n\n\nIntuition on barrier heights\nThere are other variants of the triple barrier method with dynamic barriers, but for the vanilla version, the barriers are static. Thus, in order to implement you need to have 3 ingredients for each time \\(t\\):\n\nupper barrier \\(u_t\\),\nlower barrier \\(l_t\\), and\nhorizon \\(h_t\\).\n\nIn practice, you can not have a fixed horizon \\(h_t\\) for each time \\(t\\), since markets are not 7/24 open. Based on the resolution of your data, you can either filter out the entry points that does not fit your time horizon, or you can use the maximum possible time horizon \\(h_t = \\max\\{ h^{\\text{max}}, h_t^* \\}\\), where \\(h_t^*\\) is the remaining time that the market closes\nTo calculate the upper and lower barriers, you can either use a (1) fixed value, or you can use a (2) volatility based value to calculate the barriers. I have seen cases where both methods are viable. The interpreatation of the both cases are crucial when you are in the feature engineering stage. Case (2) is scaled by the volatility, so may think of it as the effect of the volatility is removed. This could be advantageous if the trends in the market are volatility independent. Case (1) is sensitive to the volatility, so it may be more suitable if the trends in the market are volatility dependent. I usually use both cases and see which one performs better, but the crucial part is to interpret the labels correctly.\nFor Case (2), I have seen many people estimating volatility using prices, but I think it is better to use returns. Moreover, I also do not advise using the built-in standard deviation functions in your programming language. I take the square of the returns, and then I use the exponential moving average (EMA) to smooth the volatility. Taking the square of the returns means that you are asuming the expected returns are 0, which is how market operates for many assets (e.g. forex). In my experience, I have not seen much difference between using EMA or other moving average functions. The more important part is the span of those moving average functions. I usually use a span of 20, but this is another tuning parameter that you can play with.\nLet \\(\\hat{\\sigma}_t\\) be the estimated volatility of the asset. The upper and lower barriers are calculated as follows:\n\\[\n\\begin{aligned}\nu_t &= p_t \\cdot (1 + \\hat{\\sigma}_t * \\alpha) \\\\\nl_t &= p_t \\div (1 + \\hat{\\sigma}_t * \\alpha)\n\\end{aligned}\n\\]\nwhere I consider \\(\\alpha\\) as a tuning parameter. The higher the value of \\(\\alpha\\), the less \\(1\\) and \\(-1\\) labels are generated.\nFor demonstration, I will code in R, since I am using Quarto to write this article, and it’s aesthetically pleasing to read. Let’s download Nikkei 225 data from Yahoo Finance and see how it works."
  },
  {
    "objectID": "posts/3pleB/index.html#implementation-on-r",
    "href": "posts/3pleB/index.html#implementation-on-r",
    "title": "The triple barrier method",
    "section": "Implementation on R",
    "text": "Implementation on R\n\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(xts)\n\n# download data\ngetSymbols(\"^N225\", from = \"2017-01-01\", to = \"2017-12-31\")\n\n[1] \"^N225\"\n\ncolnames(N225) <- c(\"open\", \"high\", \"low\", \"close\", \"volume\", \"adjusted\")\n\n# convert to tibble and drop missing values\nalpha <- 3\nN225 <- N225 %>%\n    as.data.frame() %>%\n    mutate(date = index(N225)) %>%\n    select(date, high, low, close) %>%\n    drop_na() %>% # drop missing values\n    mutate(returns = close / lag(close) - 1) %>%\n    mutate(volatility = EMA(returns ** 2, 20) ** 0.5) %>%\n    mutate(u_bar = close * (1 + volatility * alpha), l_bar = close * (1 - volatility * alpha)) %>%\n    drop_na() %>% # drop missing values\n    as_tibble() # convert to tibble\n# take a look at the data\nhead(N225)\n\n# A tibble: 6 × 8\n  date         high    low  close   returns volatility  u_bar  l_bar\n  <date>      <dbl>  <dbl>  <dbl>     <dbl>      <dbl>  <dbl>  <dbl>\n1 2017-02-02 19171. 18867. 18915. -0.0122      0.00993 19478. 18351.\n2 2017-02-03 19061. 18831. 18918.  0.000191    0.00944 19454. 18382.\n3 2017-02-06 19076. 18899. 18977.  0.00309     0.00903 19491. 18463.\n4 2017-02-07 18971. 18805. 18911. -0.00347     0.00866 19402. 18420.\n5 2017-02-08 19009. 18876. 19008.  0.00512     0.00839 19486. 18529.\n6 2017-02-09 18991. 18875. 18908. -0.00526     0.00814 19369. 18446.\n\n\nLet’s visualize the data. At the moment I am testing out the library rtemis, a library for machine learning and visualization. It’s not on CRAN yet, but you can install it from GitHub.\n\nlibrary(plotly)\nlibrary(rtemis)\nremotes::install_github(\"zac-garland/zgtools\")\ncolorp <- rtpalette(\"imperialCol\")\nlength(colorp)\n\n[1] 26\n\ncolorp[17]\n\n$lemonYellow\n[1] \"#FFDD00\"\n\n# plot\nplot_ly(N225, x = ~date, y = ~close, type = \"scatter\", mode = \"lines\", name = \"close\", color = I(colorp[[24]])) %>%\n    add_trace(y = ~u_bar, mode = \"lines\", name = \"upper barrier\", line = list(dash = 'dot'), color = I(colorp[[7]])) %>%\n    add_trace(y = ~l_bar, mode = \"lines\", name = \"lower barrier\", line = list(dash = 'dot'), color = I(colorp[[16]])) %>%\n    layout(title = \"Nikkei 225\", yaxis = list(title = \"Price\", type = 'log'), xaxis = list(title = \"Date\"))\n\n\n\n\n\n\nlabel_3plb <- function(df, h = 10, alpha = 1) {\n    # initialize: y, ambiguity\n    df$y <- NA\n    df$ambiguity <- NA\n\n    for(i in 1:(nrow(df) - h)) {\n        # find first cross\n        u_cross <- which(df$u_bar[i] < df$high[i:(i + h)])[1]\n        l_cross <- which(df$l_bar[i] > df$low[i:(i + h)])[1]\n        \n        if (is.na(u_cross) & is.na(l_cross)) {\n            df$y[i] <- 0\n        } else if (is.na(u_cross) & !is.na(l_cross)) {\n            df$y[i] <- -1\n        } else if (!is.na(u_cross) & is.na(l_cross)) {\n            df$y[i] <- 1\n        } else if (u_cross == l_cross) {\n            df$ambiguity[i] <- TRUE\n        } else if (u_cross < l_cross) {\n            df$y[i] <- 1\n        } else {\n            df$y[i] <- -1\n        }\n    }\n    return(df)\n}\n# label the data\nN225 <- label_3plb(N225, h = 10, alpha = 3)\npaste(\"Number of 1, 0, and -1 labels:\", sum(N225$y == 1, na.rm = T), sum(N225$y == 0, na.rm = T), sum(N225$y == -1, na.rm = T))\n\n[1] \"Number of 1, 0, and -1 labels: 82 89 46\"\n\npaste(\"Percentage of ambiguity:\", sum(!is.na(N225$ambiguity)) / nrow(N225))\n\n[1] \"Percentage of ambiguity: 0\"\n\n\n\n# drop ambiguity and missing values\nN225 <- N225 %>% select(-ambiguity) %>% drop_na()\n# make y column a factor\nN225$y <- as.factor(N225$y)\n# name the labels | buy, hold, sell\nN225$y_n <- ifelse(N225$y == 1, 'Buy', ifelse(N225$y == 0, 'Hold', 'Sell'))\n\n# plot\nplot_ly(data = N225, x = ~date, y = ~close, type = 'scatter', mode = 'lines', color = I('gray'), showlegend = FALSE) %>%\n    add_trace(x = ~date, y = ~close, color = ~y_n, type = 'scatter', mode = 'markers', marker = list(size = 10, opacity = 0.8), showlegend = T, colors = c('green', 'black', 'red')) %>%\n    layout(title = 'Nikkei 225', xaxis = list(title = 'Date'), yaxis = list(title = 'Price', type = 'log'), legend = list(title = list(text = 'Label')))\n\n\n\n\n# blockker\n\n\nReferences\n\n[1] De Prado, M. (2018). Advances in financial machine learning. John Wiley & Sons."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Cem Sirin",
    "section": "",
    "text": "The triple barrier method\n\n\n\n\n\n\n\nlabelling\n\n\nmachine learning\n\n\nquantitative finance\n\n\n\n\nA detailed explanation of the triple barrier method, a method to label data for asset price prediction.\n\n\n\n\n\n\nMar 26, 2023\n\n\nCem Sirin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Overview of My Blog\n\n\n\n\n\n\n\norganization\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\nCem Sirin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#quantitative-finance",
    "href": "blog.html#quantitative-finance",
    "title": "Cem Sirin",
    "section": "Quantitative Finance",
    "text": "Quantitative Finance\nWhen it comes to quantitative finance, there is no clear set of topics that it covers. Traditionally, it leans more towards topics like Risk Management, Portfolio Management, and Asset Pricing. Although these topics are still relevant, they sprout from a more traditional view of finance as the main goal is to decrease risk and uphold a high expected return. This goal is formed by the idea that all financial assets obey to Risk-Return tradeoff, that is, the higher the expected return, the higher the risk.\nAlthough, empirically this is seems true in the long-term, it does not necessarily mean that we can not achieve higher returns in the short-term. With the rise of machine learning and more interdisciplinary approaches, many new topics have emerged. The new goal is to increase returns in the short-term, while still maintaining a low risk profile. This is where topics like algorithmic trading and high-frequency trading come into play, and which I will cover in this blog.\nSince these fields are quite new, there are no clear compartmentalized topics. One of the most influential books in this field is Advances in Financial Machine Learning by Marcos Lopez de Prado. I think the chapters in this book is a good way of compartmentalizing the topics in this field, so I will follow the same structure. And, while on the topic of books, I also quite like Machine Learning for Factor Investing by Coqueret and Guida, which is also available free online. It is a “bookdown”, which is a book written in R Markdown and I like its User Interface a lot. Anyways, here is the list of topics I will cover in this blog (I will update this list as I add more content :D ):\n\nLabelling"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Cem Sirin",
    "section": "Experience",
    "text": "Experience\n\nErasmus School of Economics\n\nTeaching Assistant | September 2021 - June 2022"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cem Sirin",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\nSapienza University of Rome\nMSc in Data Science\n\n\n\nErasmus University Rotterdam\nDouble BSc in Economics and Econometrics\n\n\n\nRobert College, Istanbul\nHigh School Diploma"
  },
  {
    "objectID": "index.html#working-experience",
    "href": "index.html#working-experience",
    "title": "Cem Sirin",
    "section": "Working Experience",
    "text": "Working Experience\n\n\n\n\n\n\n\n\n\n\nErasmus School of Economics\nTeaching Assistant\nSeptember 2021 - June 2022"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Cem Sirin",
    "section": "Contact",
    "text": "Contact\nContact me via my email: sirinc3m1 [at] gmail.com. Replace the [at] with @, and 3 with e."
  },
  {
    "objectID": "posts/overview/index.html#quantitative-finance-qf",
    "href": "posts/overview/index.html#quantitative-finance-qf",
    "title": "An Overview of My Blog",
    "section": "Quantitative Finance (QF)",
    "text": "Quantitative Finance (QF)\nWhen it comes to quantitative finance, there is no clear set of topics that it covers. Traditionally, it leans more towards topics like Risk Management, Portfolio Management, and Asset Pricing. Although these topics are still relevant, they sprout from a more traditional view of finance as the main goal is to decrease risk and uphold a high expected return. This goal is formed by the idea that all financial assets obey to Risk-Return tradeoff, that is, the higher the expected return, the higher the risk.\nAlthough, empirically this is seems true in the long-term, it does not necessarily mean that we can not achieve higher returns in the short-term. With the rise of machine learning and more interdisciplinary approaches, many new topics have emerged. The new goal is to increase returns in the short-term, while still maintaining a low risk profile. This is where topics like algorithmic trading and high-frequency trading come into play, and which I will cover in this blog.\nSince these fields are quite new, there are no clear compartmentalized topics. One of the most influential books in this field is Advances in Financial Machine Learning by Marcos Lopez de Prado. I think the chapters in this book is a good way of compartmentalizing the topics in this field, so I will follow the same structure. And, while on the topic of books, I also quite like Machine Learning for Factor Investing by Coqueret and Guida, which is also available free online. It is a “bookdown”, which is a book written in R Markdown and I like its User Interface a lot. Anyways, here is the list of topics I will cover in this blog (I will update this list as I add more content :D ):\n\nLabelling\nModel Selection"
  },
  {
    "objectID": "posts_removed/secimler1/index.html",
    "href": "posts_removed/secimler1/index.html",
    "title": "2023 Turkish Presidential Election, Part 1: The Table of Six",
    "section": "",
    "text": "As it stands, there are three potential candidates for the coalition: Kemal Kılıçdaroğlu, the leader of the Republican People’s Party (CHP), Ekrem İmamoğlu, the mayor of Istanbul, and Mansur Yavaş, the mayor of Ankara. The other five parties in the coalition seem to be in favor of Kılıçdaroğlu, but IYI Party’s leader Meral Akşener made it clear that she’d rather have İmamoğlu or Yavaş as the candidate, as the two regarded to be more popular than Kılıçdaroğlu.\nIt is quite hard to forecast elections in Turkey, as the polls are often wrong, and the election results are often unexpected. However, let’s try our best to analyze the situation with what we have publicly available. Here, I am uploading the data I have scraped from Wikipedia, you can find the code here.\n\n# Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Style\nplt.style.use('seaborn-darkgrid')\n# Read the data\nfinal_df = pd.read_pickle('polls.pkl')\n\n# Fark meaning difference, we are only interested in the difference between candidates and the incumbent (Erdoğan)\nfark_cols = [col for col in final_df.columns if \"Fark\" in col]\n\n# Scatter plot\nfig, ax = plt.subplots(figsize=(20, 10))\nfor col in fark_cols:\n    # plot the points\n    ax.scatter(final_df['Başlangıç'], final_df[col], label=col)\n    # smooth the line with exponential moving average\n    ax.plot(final_df['Başlangıç'], final_df[col].ewm(alpha=0.1).mean(), label=col + ' (EMA)')\n\n\nfor i in final_df.index:\n    y = -20 + np.random.randint(0, 7)\n    ax.axvline(final_df.loc[i, 'Başlangıç'], color='black', alpha=0.2, linestyle='--')\n    ax.annotate(final_df.loc[i, 'Anket şirketi'], (final_df.loc[i, 'Başlangıç'], y), (0, 20), textcoords='offset points', va='bottom', ha='center', fontsize=8, rotation=90)\n\n\n# annotate source wikipedia\nax.annotate('Source: Wikipedia - Opinion polling for the 2023 Turkish presidential election', (0,0), (0, -50), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.annotate('#Polls: ' + str(len(final_df)), (0,0), (0, -70), xycoords='axes fraction', textcoords='offset points', va='top', fontsize=12)\nax.legend()\nplt.show()\n\n/var/folders/bh/jd5jd5f11xz8ggqw3gx8jk6c0000gn/T/ipykernel_1428/799877934.py:6: MatplotlibDeprecationWarning:\n\nThe seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead."
  }
]